---
title: 认证、授权及准入控制和dashboard
date: 2021-01-29 08:20:16
tags:
---



#  前言

kube-apiserver集群入口（资源关联、控制；gateway），用户创建pod, 通过api server；scheduler进程也需要与api server通信，调度pod；controller进程也需要与api server通信完成获取当前集群当前状态；调度完成之后, kubelet通过api server加载pod定义；kube-proxy apiserver上的service定义转换为本地的iptables/ipvs规则。存储这些数据至etcd中。跑的pod也有可能连接api server，coredns把service放在解析记录中；dashboard也是客户端；kubectl也是api server的客户端；

![image-20210129162727446](http://myapp.img.mykernel.cn/image-20210129162727446.png)

- 认证账号
  - useraccount
    - client cert CN(binding User)
    - token：引导、静态、jwt
    - password
  - serviceaccount
    - secrets认证信息, 应该使用generic类型的证书，k8s自动生成secrets
- 授权: authorization
  - Node: 节点pod自动访问api server
  - RBAC: 角色访问控制：user/group/sa -> 角色（role:名称空间-资源-verb, clusterrole: 资源-verb)
  - ABAC

> 以上2个阶段，一票通过：每个阶段一个插件检查通过，后面插件不用检查。

- 准入控制, admission controller

  - 变异：模块中的规范，**不合规范会修改yaml定义属性值**。
  - 检验：检验yaml定义的属性值, **不合规范会拒绝用户操作**。

  > limitrange, resourcequota

> 准入控制，一票否决：一个插件不通过，后面不用检查，直接拒绝。



# 为什么需要认证、授权、准入控制

避免任何人连接api server，获取k8s资源

不同程序的有不同的权限模型



人连接apiserver

https://www.kubernetes.org.cn/doc-31

1. 认证Auth：有合理的账号。 api server通过插件引入；

   > api server启动时指定哪个认证插件  
   >
   > --client-ca-file=/etc/kubernetes/pki/ca.crt 就会启动客户端证书认证,当一个客户端证书通过认证，该证书主题的名字就被作为该请求的用户名.
   >
   > –token-auth-file=SOMEFILE, 启动Token认证。目前，Token没有有效期，必须重启API服务，Token列表的更改才会生效。
   >
   > 该文件是一个CSV文件，含有三行：Token，用户名，用户uid。
   >
   > 
   >
   > bootstrap boot: kube-apiserver --enable-bootstrap-token-auth, 必须激活控制器 kube-controller-manager --help | grep controller  ；--controllers=*,tokencleaner, 拿着令牌的人属于system:bootstrappers 组
   >
   > 

   

2. 授权：不同的人进来对不同资源有不同的权限。读写。 api server通过插件引入；

   > --authorization-mode=Node,RBAC
   >
   > AlwaysDeny会阻止所有的请求（测试中使用的）。
   >
   >    AlwaysAllow允许所有请求，如果不需要授权，可以使用这个参数。
   >
   >   ABAC（Attribute-Based Access Control）用于已经配置的用户授权规则。
   >
   > RBAC 基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。

3. 准入控制：kubectl get describe读；edit/delete/apply有写操作。 api server通过插件引入；

   - 定义用户创建pod的cpu核心数不能超过2个核心。**检查权限对应的细节行为**。
   - 用户apply的yaml文件字段不全，准入控制时以默认值**补全yaml文件**。

   > --enable-admission-plugins=NodeRestriction

api server启动后就有这些功能, 如果在**认证或授权或准入控制时**有多个插件，使用哪个插件进行？ 自上而下的检查，一个通过则表示通过认证。

<!--more-->



## k8s用户

### 人类用户 useraccount

可以通过认证插件client cert(CN)来认证, ....

### 服务账号(Service Account)

**k8s集群之上的Pod中的**进程连接api server使用的账号

```bash
kubectl explain sa

secrets # 加载自已的认证信息认证

spec.imagepullsecrets
```



<!--more-->

# 认证插件

client certificates X509

password

plain tokens 

bootstrap tokens 引导令牌，构建集群或节点加入集群时使用。

jwt token: json web token, http协议携带令牌

所以插件都不认识，system:anonymous用户或system:unauthenticated组

## client certificates X509

k8s 认证是双向的，kubectl 提交自已(client cert)证书给api server, 并请求api server证书(server cert)。

> 信任机构颁发
>
> 证书申明的身份和持有者身份一致
>
> 证书在有效期内

需要自已CA，给api server发证书，给与api server通信的 kubectl、scheduler、controller、kubelet、kube-proxy、dashboard、coredns、...发证书

不能使用同一个证书，因为认证通过后，还需要通过认证的subject /CN=$NAME 来识别你是谁，分用户来认证。

二进制部署证书制作：ca, ..., 且CN用户名必须正确，与K8S内建的用户名保持一致。

suject /O=$group 用户所属的组

kubectl 认证使用client 证书，把证书和私钥放在kubectl配置文件中~/.kube/config, 第一次kubeadm自动生成/etc/kubernetes/admin.conf

## password

用户名和密码通过base64编码后，放在https协议的首部来承载

api server启动时加载用户和密码文件

## plain tokens

不需要用户名，只需要令牌即可，谁持有令牌就可以了. http承载

## jwt tokens

客户端认证使用

## bootstrap token

构建集群时使用



# 授权

- subjects
  - User Account
  - Service Account
- verb
- Object
  - Resources
  - subResources
  - URL 非资源型URL
- Role,RoleBinding
  - Role: 定义**verb -> Object**
  - RoleBinding: 定义**subjects -> Role**

这种授权，将来不需要动user/group/object, 仅需要修改绑定

K8S有集群级资源和名称空间级资源 

- Role/ClusterRole

  - Role: 名称空间级的Role, pod, service, controller, ...

  - ClusterRole: 集群级的Role, ns, pv, node...

    > 集群级的角色还能包含名称空间，集群角色可操作pod, 即所有名称空间的pod均可操作

- Rolebinding/ClusterRolebinding

  - Rolebinding 用户/组绑定至Role， 操作特定名称空间资源

    > 绑定用户至clusterrole( create pod), 局限权限为**指定名称空间创建pod**
    >
    > 使用名称空间直接rolebind + role + user, 为什么使用rolebind + clusterrole + user?
    >
    > 授权30个名称空间至不同的用户。使用第1种方法需要30个role. 使用第2种方法仅需要一个clusterrole.

  - ClusterRolebinding  用户/组绑定至ClusterRole，操作整个集群范围内的资源

    > 绑定用户至clusterrole( create pod),所有名称空间创建pod
    >
    > 注意：clusterrolebinding不能绑定role来升级权限

k8s支持多个角色权限聚合

## RBAC

资源的管理：CURD映射成HTTP协议的GET、PUT、POST、PATCH、DELETE

kubectl发出的请求包含类似以下信息

- user

- group
- extra 额外属性
- api     
- namespace
- api group
- api
- resource
  - subresource 子资源 status
- request path 相对api来讲，请求的URL路径是什么。即资源url标识
- http request verb: 请求方法
- api request verb: 向k8s api请求方法

kubectl create API动作 对应 HTTP的请求动作PUT

资源操作对特定namespace-api group-api-resource的请求

授权插件根据user、group 对某个resource/subresource有什么API Verb（操作）

> 主-谓-宾
>
> 主：user/group
>
> 谓: api verb
>
> 宾：resource/subresource
>
> 1. 资源操作 主-谓-宾 RBAC
> 2. 资源字段能否修改 ABAC

授权插件

- abac k8s V1.6-  基于属性的访问控制，一个用户能操作哪个字段？
- **rbac**  k8s V1.7+ 基于角色 的访问控制，根据user、group 对某个resource/subresource有什么API Verb（操作）. 权限授于角色，`人或组`关联至`角色`就有权限。
- webhook: http回调函数，http资源变化，触发器触发代码操作
- **node**: pod调度结果对node授权，自动完成

授权是1票通过，自上而下检查权限，一个权限通过即通过授权（1票通过机制）

```diff
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
+    - --authorization-mode=Node,RBAC # K8S的授权插件
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt # etcd的ca
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt # api server连接etcd , 使用etcd颁发的证书 4个
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key #
    - --etcd-servers=https://127.0.0.1:2379 # etcd服务
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key

```

定义RBAC授权

1. 主语
   - 角色(权限赋于角色)
   - 用户/组
2. 对象
   - 资源/子资源
3. api verb 操作列表 http对应kubectl
   - post - create
   - get/head - get, list
   - put - update
   - patch - patch
   - delete - delete, deletecollection



## rolebinding

定义权限时，定义角色

1. 拥有什么资源
2. 拥有什么权限

授权即把认证的用户useraccount(**user/group    CN/O**)/serviceaccount(**name即用户名**)与角色关联， 即user对资源有权限, k8s中叫**`rolebinding`**



## 用户是否有权限

```bash
# 对指定资源操作？
kubectl auth can-i create deployments --namespace dev

# 对指定资源列出？
kubectl auth can-i list deployments --namespace dev
```



# 准入控制器

创建pod启动后，没有定义存储卷，也有一个存储卷

```bash
[root@master chapter9]# kubectl describe pod/pod-demo

    Mounts:
    	# 而且挂载，令牌如何使用
    	# 保存pod serviceaccount账号认证apiserver的认证信息
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8qddj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
	# 有卷
  default-token-8qddj: # 默认令牌，让当前pod能连接ApiServer更新自已的信息
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-8qddj
    Optional:    false


[root@master chapter9]# kubectl exec -it pod-demo -- sh
/ # ls /var/run/secrets/kubernetes.io/serviceaccount -l
total 0
lrwxrwxrwx    1 root     root            13 Jan 18 02:44 ca.crt -> ..data/ca.crt
lrwxrwxrwx    1 root     root            16 Jan 18 02:44 namespace -> ..data/namespace
lrwxrwxrwx    1 root     root            12 Jan 18 02:44 token -> ..data/token


```

没有指定service account,  创建pod时，准入控制器会自动给pod赋于一个serviceAccountName, 自动把服务账户信息以存储卷的方式，让pod挂载。所以pod的守护进程需要连接api server时，拿sa认证。就能获取自已应该获得的最小权限。



## 准入控制器

https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceautoprovision

| 准入控制插件           | 作用                                                         | 使用场景                                                     |
| ---------------------- | ------------------------------------------------------------ | :----------------------------------------------------------- |
| AlwaysAdmit（废弃）    | 违反规则不拒绝，只是记录。                                   | 测试时使用                                                   |
| AlwaysDeny（废弃）     | 无论正确与否，都拒绝。                                       | 测试时使用                                                   |
| AlwaysPullImages       | pod的imagepullpolicy 无论定义什么值，都将拉镜像。            | 强制不使用本地镜像   优点：避免别人把镜像置入后门，容器安全。缺点：消耗带宽，启动速度慢。 |
| DefaultStorageClass    | pv.spec.storageclass指定就属于一个类，不指定就不属于存储类。 pvc.spec.storageclass 指定自动声明pvc的存储类，不指定时，不需要存储类，直接找pv. | 配置这个时pv/pvc可以有默认存储类。可能没有启用               |
| LimitRanger            | **为 Pod 设置默认资源请求和限制，需要在 namespace 中创建一个 LimitRange 对象**<br/>request资源下限运行：pod必须有的资源。  <br/>limits资源硬限制、上限：pod运行不需要立即占用这么多，可以运行慢慢申请。   <br/><br/>资源下限的作用，调度pod时其资源最低需求小于余剩的空间 (节点总量-节点的所有pod的最低资源需求和) , cpu可以超配，内存不能超配。 | pod 计算资源限制（pod容器能使用多少cpu/ram资源） <br/> 创建名称空间中的limitRange资源，限制**1. 任何名称空间中的pod都应该有资源下限和上限定义，没有就有default定义。2. 定义的资源限制必须在规范的区间。**<br/>LimitRanger检查拒绝，就不能定义Pod. <br/>优点：名称空间配置limitRange, 不存在pod超配使用资源。<br/>缺点：**限制单个pod不能超2核。创建过多pod, 一个名称空间吞掉所有资源。使用resourcequota结合使用**。 |
| ResourceQuota          | 限制 Pod 的请求不会超过配额，需要在 namespace 中创建一个 ResourceQuota 对象 | 总量上限制k8s可用资源: 存储、计算<br/>1. namespace跑几个pod, svc, deploy, pvc, ....。<br/>2. 所有pod加起来cpu/ram问题<br/>资源生效由ResourceQuota检验 |
| ServiceAccount         | 创建的pod默认有serviceaccount, default. secret卷关联至pod认证到api | pod自定义认证到api server                                    |
| SecurityContextDeny    |                                                              | 拒绝包含非法 SecurityContext 配置的容器                      |
| NodeRestriction        | 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制。<br/>限制kubelet仅可访问node、endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源（仅适用于v1.7+) |                                                              |
| PodSecurityPolicy      | PSP简称                                                      | 该插件用于创建和修改pod，使用Pod Security Policies时需要开启。设定pod不能使用hostnetwork: true.<br/>你启动了PSP, 得先定义好基本资源，确保flannel pod可以启动。 |
| NamespaceAutoProvision |                                                              | 名称空间不存在，自动创建                                     |

启用准入控制器

```diff
[root@master ~]# cat /etc/kubernetes/manifests/kube-apiserver.yaml 
  - command:
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
+    - --enable-admission-plugins=NodeRestriction # 没有定义的，也有默认启用的状态, 不需要随意加上PSP，加上整个集群不能使用。
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key

```

默认启用的

https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default

```shell
NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, RuntimeClass, CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota
```



## LimitRange and LimitRanger

pod没有资源限制，创建ns.LimitRange对象后, ns中会默认添加资源。创建pod时会修改资源请求，如果指定了资源就不会修改

- 施加容器上，pod中每个容器限制
- pod级别，pod中所有容器资源之和的限制
- pvc级别，pvc限制



limitrange只是限制单个级别(容器、pod、pvc)资源限制，但是无法阻止用户创建巨量的pod, 占据整个系统上的计算资源。

- min <=**request** <=  **limits**<= max
- maxlimitrequestratio: **limits/requests**

### 示例

https://kubernetes.io/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B

```yaml
[root@master ~]# cat ./Kubernetes_Advanced_Practical/chapter10/limitrange-demo.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range # 限制cpu维度
  namespace: myns
spec:
  limits: # 限制
  - type: "Pod" # Pod级别
    max:                      # 上限，限制不能超过
    	cpu: 2
    	memory: 1gi
    min:                      # 下限，请求不能低于
    	cpu: 200m
    	memory: 6Mi	
  - type: "Container" # 每个容器
    max:
    	cpu: 2
    	memory: 1gi
    min:
    	cpu: 100m
    	memory: 4Mi	
    default:                 # 在min,max之间启动容器没有设定max时，自动填充
    	cpu: 300m
    	memory: 200Mi
    defaultRequest:          # 启动容器没有设定min时，自动填充
    	cpu: 200m
    	memory: 100Mi
    maxLimitRequestRatio: # 上限/下限的比值不能超过这个限制
      cpu: 20 # 2000m/100m=20

```

> 指定方式：
>
> 1、 指定min,max
>
> 2、指定min,maxLimitRequestRatio
>
> 3、指定min,max,maxLimitRequestRatio



```bash
[root@master ~]# kubectl create ns myns
namespace/myns created

[root@master chapter10]# cat limitrange-demo.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1000m # 默认requests
    defaultRequest:
      cpu: 1000m # 默认limits
    min:
      cpu: 500m
    max:
      cpu: 2000m
    maxLimitRequestRatio:
      cpu: 4
    type: Container

[root@master chapter10]# kubectl -n myns apply -f limitrange-demo.yaml
limitrange/cpu-limit-range created


[root@master chapter10]# kubectl get limitrange -n myns
NAME              CREATED AT
cpu-limit-range   2021-02-01T07:00:46Z


[root@master chapter10]# kubectl describe limitrange -n myns 
Name:       cpu-limit-range
Namespace:  myns
Type        Resource  Min   Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---  ---------------  -------------  -----------------------
Container   cpu       500m  2    1                1              4

```

创建pod测试

```diff
[root@master chapter10]# kubectl get pod -n myns
NAME                         READY   STATUS    RESTARTS   AGE
limitrange-default-cpu-pod   1/1     Running   0          11s
[root@master chapter10]# kubectl get pod -n myns limitrange-default-cpu-pod -o yaml
  containers:
  - image: ikubernetes/myapp:v1
    imagePullPolicy: IfNotPresent
    name: myapp
+    resources:
+      limits:
+        cpu: "1"
+      requests:
+        cpu: "1"
```

### requests 小于 min

创建pod，定义requests小于LimitRange的 min 500m

```diff
"limitrange-default-cpu-pod.yaml" 8L, 138C                                                                                                                                                                                                                                                               1,1           All
apiVersion: v1
kind: Pod
metadata:
  name: limitrange-default-cpu-pod
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
    resources:
      requests:
+        cpu: "200m"
~                        
```

```bash
[root@master chapter10]# kubectl apply -f limitrange-default-cpu-pod.yaml -n myns
Error from server (Forbidden): error when creating "limitrange-default-cpu-pod.yaml": pods "limitrange-default-cpu-pod" is forbidden: [minimum cpu usage per Container is 500m, but request is 200m, cpu max limit to request ratio per Container is 4, but provided ratio is 5.000000]
# defaultRequest: cpu: 1000m # 默认limits是1000，给定200，是5倍。 超出范围
# 最小500m, 请求200m核，所以报错
```

```diff
"limitrange-default-cpu-pod.yaml" 11L, 184C                                                                                                                                                                                                                                                              11,9          All
apiVersion: v1
kind: Pod
metadata:
  name: limitrange-default-cpu-pod
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
    resources:
+      requests:
+        cpu: "500m"
+      limits:
+        cpu: "1500m"

```

> limit / request = 3 < 4(maxLimitRequestRatio) 正常
>
> requests 500m >= min 500m
>
> limits 1500m <= max 2000m

```bash
[root@master chapter10]# kubectl apply -f limitrange-default-cpu-pod.yaml -n myns
pod/limitrange-default-cpu-pod created
```



### limit 大于max

```diff
"limitrange-default-cpu-pod.yaml" 11L, 189C                                                                                                                                                                                                                                                              10,7          All
apiVersion: v1
kind: Pod
metadata:
  name: limitrange-default-cpu-pod
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
    resources:
+      limits:
+        cpu: "3"

```

```bash
[root@master chapter10]# kubectl apply -f limitrange-default-cpu-pod.yaml -n myns
Error from server (Forbidden): error when creating "limitrange-default-cpu-pod.yaml": pods "limitrange-default-cpu-pod" is forbidden: maximum cpu usage per Container is 2, but limit is 3

```



## Resource Quota

limitrange只是限制单个级别(容器、pod、pvc)资源限制，但是无法阻止用户创建巨量的pod, 占据整个系统上的计算资源。

所以需要Resource Quota





https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/#%E5%AD%98%E5%82%A8%E8%B5%84%E6%BA%90%E9%85%8D%E9%A2%9D

limits.cpu 只计算运行状态的pod的cpu总额不能超过这个总额

limits.memory   只计算运行状态的pod的ram总额不能超过这个总额

requests.cpu                 cpu下限总和不能超过这个

requests.memory        ram下限总和不能超过这个



无存储类，即所有

quests.storage 所有pvc请求资源空间量不可以大于这个

pvc：                 pvc数目不能超过这个

存储类上限制

`<storage-class-name>.storageclass.storage.k8s.io/requests.storage` 存储类申请大小限制
`<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims` 存储类申请pvc个数

| 资源名称                     | 描述                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| `requests.ephemeral-storage` | 在命名空间的所有 Pod 中，本地临时存储**请求****的总和**不能超过此值。 |
| `limits.ephemeral-storage`   | 在命名空间的所有 Pod 中，本地临**时存储限制值的总和**不能超过此值。 |
| `ephemeral-storage`          | 与 `requests.ephemeral-storage` 相同。                       |

示例：计算资源限制 pod总量限制

```bash
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  # namespace 指定
spec:
  hard:
    pods: "4"
    requests.cpu: "1"
    requests.memory: 1Gi
    requests.ephemeral-storage: 2Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    limits.ephemeral-storage: 4Gi
```



### Object Count Quota

https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/#%E5%AF%B9%E8%B1%A1%E6%95%B0%E9%87%8F%E9%85%8D%E9%A2%9D

k8s v1.9之后可以对任何资源进行数量限制

指定名称空间上pod总数、deploy总数

- `count/persistentvolumeclaims`
- `count/services`
- `count/secrets`
- `count/configmaps`
- `count/replicationcontrollers`
- `count/deployments.apps`
- `count/replicasets.apps`
- `count/statefulsets.apps`
- `count/jobs.batch`
- `count/cronjobs.batch`
- `count/*` 任意资源数量

配额生效QoS 的位置，没有指定，所有表示

| 作用域           | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| `Terminating`    | 匹配所有 `spec.activeDeadlineSeconds` 不小于 0 的 Pod。      |
| `NotTerminating` | 匹配所有 `spec.activeDeadlineSeconds` 是 nil 的 Pod。        |
| `BestEffort`     | 匹配所有 Qos 是 BestEffort 的 Pod。                          |
| `NotBestEffort`  | 匹配所有 Qos 不是 BestEffort 的 Pod。                        |
| `PriorityClass`  | 匹配所有引用了所指定的[优先级类](https://kubernetes.io/zh/docs/concepts/configuration/pod-priority-preemption)的 Pods。 |

> QoS Class: 字段的值

示例： 计数限制

```bash
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
  # namespace 指定
spec:
  hard: # 硬限制
    configmaps: "10"
    persistentvolumeclaims: "4"
    pods: "4"
    replicationcontrollers: "20"       # pod控制器 v1.4之前版本
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
    
```



## 示例quota

```bash
[root@master chapter10]# cat resoucequota-demo.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-example
  # ns
spec:
  hard:
    pods: "5"
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    count/deployments.apps: "2" # 数量限制
    count/deployments.extensions: "2"
    persistentvolumeclaims: "2"

```

以上的limitrange启动了myns有下限500m, 上限1500m正常

```bash
[root@master chapter10]# kubectl get pods -n myns
NAME                         READY   STATUS    RESTARTS   AGE
limitrange-default-cpu-pod   1/1     Running   0          27m

```

```bash
# 添加quota
[root@master chapter10]# kubectl apply -f resoucequota-demo.yaml -n myns
resourcequota/quota-example created

```

```bash
# 检查
[root@master chapter10]# kubectl get resourcequota -n myns
NAME            AGE   REQUEST                                                                                                                                                LIMIT
quota-example   75s   count/deployments.apps: 0/2, count/deployments.extensions: 0/2, persistentvolumeclaims: 0/2, pods: 1/5, requests.cpu: 500m/1, requests.memory: 0/1Gi   limits.cpu: 1500m/2, limits.memory: 0/2Gi
[root@master chapter10]# kubectl describe resourcequota -n myns
Name:                         quota-example
Namespace:                    myns
Resource                      Used   Hard
--------                      ----   ----
count/deployments.apps        0      2
count/deployments.extensions  0      2
limits.cpu                    1500m  2
limits.memory                 0      2Gi
persistentvolumeclaims        0      2
pods                          1      5
requests.cpu                  500m   1
requests.memory               0      1Gi
```

创建pod, 让之和大于限制

```diff
[root@master chapter10]# cat limitrange-default-cpu-pod.yaml
apiVersion: v1
kind: Pod
metadata:
+  name: limitrange-default-cpu-pod-2
spec:
  containers:
+  - name: myapp2
    image: ikubernetes/myapp:v1
    resources:
      requests:
+        cpu: "1000m"
        memory: "600m"
+      limits:
        cpu: "1500m"
+        memory: "1000m"


```

```bash
[root@master chapter10]# kubectl apply -f limitrange-default-cpu-pod.yaml -n myns
Error from server (Forbidden): error when creating "limitrange-default-cpu-pod.yaml": pods "limitrange-default-cpu-pod-2" is forbidden: exceeded quota: quota-example, requested: limits.cpu=1500m,requests.cpu=1, used: limits.cpu=1500m,requests.cpu=500m, limited: limits.cpu=2,requests.cpu=1

# exceeded quota超过quota. 当前请求：限制1.5核和请求1核。但是quota已经使用了1.5核，请求了500毫核。限制是2核，请求1核

[root@master chapter10]# kubectl describe resourcequota -n myns
Name:                         quota-example
Namespace:                    myns
Resource                      Used   Hard
--------                      ----   ----
count/deployments.apps        0      2
count/deployments.extensions  0      2
limits.cpu                    1500m  2
limits.memory                 0      2Gi
persistentvolumeclaims        0      2
pods                          1      5
requests.cpu                  500m   1
requests.memory               0      1Gi

```

修改

```diff
"limitrange-default-cpu-pod.yaml" 15L, 275C                                                                                                                                                                                                                                                              15,9          All
apiVersion: v1
kind: Pod
metadata:
  name: limitrange-default-cpu-pod-2
spec:
  containers:
  - name: myapp2
    image: ikubernetes/myapp:v1
    resources:
      requests:
+        cpu: "500m" # 限制上已经使用500m, + 500m不超限制
        memory: "600m"
      limits:
+        cpu: "500m" # 限制上已经使用1500m, + 500m不超限制
        memory: "1000m"

```

```bash

[root@master chapter10]# kubectl apply -f limitrange-default-cpu-pod.yaml -n myns
pod/limitrange-default-cpu-pod-2 configured
[root@master chapter10]# kubectl describe resourcequota -n myns
Name:                         quota-example
Namespace:                    myns
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        0     2
count/deployments.extensions  0     2
limits.cpu                    2     2 # cpu已经满了
limits.memory                 1     2Gi
persistentvolumeclaims        0     2
pods                          2     5
requests.cpu                  1     1 # 满了
requests.memory               600m  1Gi

```



## PodSecurityPolicy PSP

要启用，得事先准许默认的pod启动的策略



将安全上下文定义在一起

普通pod

```bash
[root@master chapter10]# cat psp-restricted.yaml 
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
spec:
  privileged: false # 所有pod不能运行为特权模型
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes: # 允许哪些存储卷
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false # 允许hostnetwork? flannel一定要使用hostnetwork
  hostIPC: false #
  hostPID: false #
  runAsUser: # 允许哪个用户运行
    rule: 'MustRunAsNonRoot' # 不能使用root运行
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false

```

默认法则, 特权pod

```bash
[root@master chapter10]# cat psp-privileged.yaml 
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
spec:
  privileged: true 
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - '*'
  volumes:
  - '*'
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

```

系统级serviceaccount与特权pod关联

普通级serviceaccount与普通pod关联

```bash
[root@master chapter10]# cat clusterrole-with-psp.yaml 
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp:restricted # 受限
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - restricted # 受限
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp:privileged # 特权pod
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - privileged # 特权

# 用户绑定至集群角色就受限
[root@master chapter10]# cat clusterrolebinding-with-psp.yaml 
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: restricted-psp-user
roleRef:
  kind: ClusterRole
  name: psp:restricted
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  apiGroup: rbac.authorization.k8s.io
  name: system:authenticated # 内建的组，认证的组
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: privileged-psp-user # 特权
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:privileged
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters # 管理员
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:node # node 
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:kube-system #

```

先应用以上角色和绑定，后启用PSP准入控制，默认pod才跑得起来





# 集群组件的ssl通信

## etcd证书 4个

etcd之间使用证书, etcd是服务端，api server是etcd的客户端，需要存数据。

![image-20210129175417317](http://myapp.img.mykernel.cn/image-20210129175417317.png)

> api server与etcd通信，保存集群状态
>
> etcd每个节点有一个server cert
>
> api server有一个client cert



确保etcd中的数据不被非授权访问，做etcd的ca.

```bash
    - etcd
    - --advertise-client-urls=https://172.16.1.100:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt # 为每一个节点发一个服务端证书，给api server发一个客户端证书 3个
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://172.16.1.100:2380
    - --initial-cluster=master.magedu.com=https://172.16.1.100:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.16.1.100:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://172.16.1.100:2380
    - --name=master.magedu.com
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt # 节点间通信，对等证书 2个
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # etcd ca 让各节点间通信
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt  # etcd的ca 1个
```

api server与etcd通信时，拿着客户端证书认证到etcd上

etcd为服务端

api server为客户端

```bash
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt # etcd的ca
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt # api server连接etcd , 使用etcd颁发的证书 4个
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key #
    - --etcd-servers=https://127.0.0.1:2379 # etcd服务
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key

```



## 第2套证书 k8s证书 7个

![image-20210129181430561](http://myapp.img.mykernel.cn/image-20210129181430561.png)

k8s ca, api server作为服务端

5个客户端：controller, scheduler, proxy, kubelet, kubectl

```diff
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
+    - --client-ca-file=/etc/kubernetes/pki/ca.crt         # kubernetes ca 1个
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt # etcd的ca
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt # api server与etcd通信的客户端证书 4个
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key #
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
+    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt # kubelet连接apiserver 3个
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
+    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt # api server服务端证书 2个
+    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
```

> 除了api server都是客户端证书
>
> 证书中的cn名不一样



api server需要指挥kubelet调用docker引擎启用pod， 也需要证书；这个会自动生成

> api server是客户端
>
> kubelet是服务端



## 第3套证书

![image-20210129182828938](http://myapp.img.mykernel.cn/image-20210129182828938.png)

api server定义了当前k8s支持哪些种资源类型：service、pod、deployment、statefulset、...，也叫数据存储方案。

如果k8s内建的数据存储方案不够用，我们要自建资源类型（**扩展K8S系统**）

1. api server拿到修改go源码，缺陷。k8s迭代这么快，你修改v1.13好源码，别人已经更新v1.15了。除非你是K8S专家

2. 自建api server, 这个程序引入新的资源类型，pod方式运行在k8s之上。operator 自定义控制器

   > api server与原本api server没有关系，k8s的api server为了方便用户扩展自定义api server, 设计上就分层。
   >
   > 整个api server内部是2个部分，1个是kube-aggregator, 第2个才是kube-apiserver本身。因此客户端访问时先到达aggregator, 然后aggregator反代至内建的api server，如果自已做了一个api server, 仅需要配置aggregator反代时，内建资源到内建api server, 自定义的资源反代至自定义的api server.



自建api server与原生的api server中的aggregator安全对接



```diff
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt         # kubernetes ca 1个
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt # etcd的ca
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt # api server与etcd通信的客户端证书 4个
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key #
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt # kubelet连接apiserver 3个
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
+    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt # 代理客户端
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
+    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt # 代理ca
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt # api server服务端证书 2个
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
```



## 验证sa的证书

```diff
    - kube-apiserver
    - --advertise-address=172.16.1.100
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt         # kubernetes ca 1个
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt # etcd的ca
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt # api server与etcd通信的客户端证书 4个
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key #
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt # kubelet连接apiserver 3个
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt # 代理客户端
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt # 代理ca
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
+    - --service-account-key-file=/etc/kubernetes/pki/sa.pub # 验证sa的公钥
+    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key # 验证sa的私钥
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt # api server服务端证书 2个
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
```



# 创建认证

## kubeconfig(client cert)

用户通过kubectl连接api server, kubectl需要指定证书信息，保存于kubeconfig文件中

kubeconfig文件组成

- users 用户列表，
- clusters 集群列表
- contexts 用户与集群关联
- current-context 当前哪个用户哪个集群

```bash
[root@master chapter9]# kubectl config -h
Modify kubeconfig files using subcommands like "kubectl config set current-context my-context"

 The loading order follows these rules:

  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes
place.
  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for
your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When
a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the
last file in the list.
  3.  Otherwise, ${HOME}/.kube/config is used and no merging takes place.

Available Commands:
  current-context 配置文件中当前上下文
  delete-cluster  Delete the specified cluster from the kubeconfig
  delete-context  Delete the specified context from the kubeconfig
  delete-user     Delete the specified user from the kubeconfig
  get-clusters    配置文件中所有集群
  get-contexts    配置文件中所有
  get-users       配置文件中所有用户
  rename-context  Renames a context from the kubeconfig file.
  set             Sets an individual value in a kubeconfig file
  set-cluster     Sets a cluster entry in kubeconfig
  set-context     Sets a context entry in kubeconfig
  set-credentials Sets a user entry in kubeconfig
  unset           Unsets an individual value in a kubeconfig file
  use-context     Sets the current-context in a kubeconfig file
  view            Display merged kubeconfig settings or a specified kubeconfig file

```

查看kubeadm构建集群时生成的admin用户的kubectl配置文件

```bash
[root@master chapter9]# cat /etc/kubernetes/admin.conf 
apiVersion: v1
clusters: # 集群列表
- cluster:
    certificate-authority-data:  # 集群apiserver信任的ca的证书
    server: https://172.16.1.100:6443 # api地址
  name: kubernetes # 集群名
contexts:
- context: # 用户和集群关联
    cluster: kubernetes 
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes # 当前哪个用户哪个集群
kind: Config
preferences: {}
users:
- name: kubernetes-admin # 用户名
  user:
    client-certificate-data:   # 用户证书 base64加密结果
    client-key-data: 

```

使用命令查看

```bash
[root@master chapter9]# kubectl --kubeconfig=/etc/kubernetes/admin.conf config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.16.1.100:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```

> --kubeconfig指定kubectl配置文件，不指定默认读取~/.kube/config文件



### 制作账户

不使用默认的admin.conf来认证k8s

1. 证书：使用k8s信任ca颁发证书
2. 令牌
3. 密码

```bash
# k8s信任ca
[root@master chapter9]# ls /etc/kubernetes/pki/ca.* -l
-rw-r--r--. 1 root root 1066 Jan 14 18:34 /etc/kubernetes/pki/ca.crt
-rw-------. 1 root root 1675 Jan 14 18:34 /etc/kubernetes/pki/ca.key

# CN代表用户名，用户即对应权限， 查看CN
openssl x509 -noout -subject  -in /etc/kubernetes/pki/etcd/peer.crt 
 
# CN不需要K8S存在这个用户
```



生成私钥

```bash
openssl genrsa -out ilinux.key 2048
```

为用户生成签署请求

```bash
openssl req -new -key ilinux.key -out ilinux.csr -subj "/CN=ilinux/O=kubeusers"
```

> 用户随意
>
> kubeusers组, k8s不存在，存在的即有对应的权限

使用apiserver的ca签署

```bash
openssl x509 -req -in ilinux.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out ilinux.crt -days 3650
```

> 指定ca的证书和私钥
>
> 签署证书自创序列号文件
>
> 签署天数
>
> 签署成功就可以删除csr文件



新建kubeconfig，并生成集群

```bash
kubectl config set-cluster -h
集群名 ca文件
--embed-cert 打包文件, 证书隐藏
```

```bash
kubectl config set-cluster mykube --server="https://172.18.0.70:6443" --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-cert=true  --kubeconfig=/tmp/ilinux.kubeconfig

# 验证
kubectl config view  --kubeconfig=/tmp/ilinux.kubeconfig
```

> 添加--kubeconfig=/tmp/ilinux.kubeconfig之前，默认保存在kubectl当前读的配置文件中，添加之后将新生成配置文件保存

添加用户

```bash
kubectl config set-credentials 用户标识 客户端证书 客户端私钥 用户名 [密码] [token]
```

> 由于此处使用证书认证，所以仅提供证书、CN的名即为用户名
>
> 也可以使用密码、token认证

```bash
kubectl config set-credentials ilinux --client-certificate=ilinux.crt --client-key=ilinux.key --username=ilinux --embed-certs=true --kubeconfig=/tmp/ilinux.kubeconfig

# 验证
kubectl config view  --kubeconfig=/tmp/ilinux.kubeconfig
```

> set-credentials ilinux 后面指的仅是kubeconfig里面使用的User，与binding的名字无关
>
> --username=指定证书的CN名, 也是binding指定的subjects中的User的名字
>
> 添加--kubeconfig=/tmp/ilinux.kubeconfig之前，默认保存在kubectl当前读的配置文件中，添加之后将新生成配置文件保存

用户与集群关联起来

```bash
kubectl config set-context 名 集群标签 用户标识
```

```bash
kubectl config set-context ilinux@mykube --cluster=mykube --user=ilinux --kubeconfig=/tmp/ilinux.kubeconfig


# 验证
kubectl config view  --kubeconfig=/tmp/ilinux.kubeconfig
```

> 添加--kubeconfig=/tmp/ilinux.kubeconfig之前，默认保存在kubectl当前读的配置文件中，添加之后将新生成配置文件保存

当前用户

```bash
kubectl config use-context ilinux@mykube

# 验证
kubectl config view  --kubeconfig=/tmp/ilinux.kubeconfig
```

使用kubectl

```bash
kubectl get pod  --kubeconfig=/tmp/ilinux.kubeconfig
```

> 用户没有权限获取默认名称空间中的pod资源 

将以上--kubeconfig去掉将添加至默认配置文件~/.kube/config文件中，以下是使用添加的用户至默认配置文件中



## 创建serviceaccount

k8s自动使用令牌认证机制给账号赋于账号信息，创建账号没有权限



```bash
kubectl create serviceaccount sa-demo --dry-run -o yaml

# 验证
kubectl get sa

#创建pod没有指定sa, 使用default sa
kubectl describe sa default
secrets: default-token-krtfq

#查看secret
kubectl get secrets
NAME TYPE DATA AGE



```

# 创建授权

完成名称空间

- rolebinding: role(属于名称空间) + user, 降级clusterrole(不属于名称空间) + user

完成集群

- clusterrolebinding: clusterrole + user



- 对象
  - 集群级别 node
  - 名称空间 pod
  - 非资源 /healthz

角色的Rules定义

- 资源， 列表方式
  - resources 哪个类型的资源
  - resourceNames 哪个具体的资源
  - nonResourceURLs 非资源型的URL
- 操作，列表方式或*所有
  - verbs

binding: 角色 + subjects (useraccount/serviceaccount)

## 名称空间

### 角色

```bash
#chapter10/pods-reader.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1 # 组/版本
metadata:
  namespace: testing # 哪个名称空间中的资源，role也属于名称空间
  name: pods-reader  # role名
rules: # 没有spec, endpoint/ 定义规则 
- apiGroups: [""]   # "" 表示core API group；* 所有组； apps/v1 表示这个组/版本, # 不同的组有不同版本的资源，界定哪个组中的资源
  resources: ["pods", "pods/log"] # 什么资源，什么子资源
  verbs: ["get", "list", "watch"] # 资源操作, get/list/watch -w
```

> pods-reader角色表示：可以对ns中apigroups下的resources有哪些verbs

例如：授权用户读default名称空间的pod/service

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1 # 组/版本
metadata:
  namespace: default
  name: res-reader
rules:
- apiGroups: [""]   
  resources: ["pods", "services"] 
  verbs: ["get", "list", "watch"] 
```



```bash
# 应用
apply
# 验证
kubectl get role -n default
# 引用同名称空间下的资源
```

### 角色绑定

```bash
#chapter10/read-pods.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: testing # 绑定在哪个名称空间下，引用同名称空间下的role
subjects: # 一个或一组账号（user/serviceaccount/group)
- kind: User # 用户User, 组Group, 服务账号ServiceAccount
  name: kube-user1 # openssl -subj 定义用户名或组名或服务账号名. 与kubeconfig定义的用户名无关
  apiGroup: rbac.authorization.k8s.io # 哪个api群组； serviceaccount可以为空。user/group默认是rbac.authorization.k8s.io
  #namespace 用户不在名称空间，可以为空； sa在名称空间，必须写表示哪个名称空间的sa
roleRef: # 一个角色
  kind: Role
  name: pods-reader # role名或clusterrole名
  apiGroup: rbac.authorization.k8s.io
```

> 把一个或一组账号绑定在一个角色上

例如：允许ilinux用读default名称空间的pod/service

由于以上已经创建角色：res-reader, 现在只需要绑定用户

```yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ilinux-res-reader
  namespace: default # 引用同名称空间的role
subjects:
- kind: User
  name: ilinux
  # 不需要写api群组
  # 不需要namespace
roleRef:
  kind: Role
  name: res-reader
  apiGroup: rbac.authorization.k8s.io
```

```bash
# 应用
apply
# 验证
kubectl get rolebinding -n default
# 引用同名称空间下的role
```



### 验证用户

```bash
kubectl get pod --kubeconfig=/tmp/ilinux.kubeconfig
# 成功


kubectl get deploy --kubeconfig=/tmp/ilinux.kubeconfig
# 失败，因为仅授权了pod
```

现在要继续授权这个用户

1. 仅需要修改role, 因为已经binding过了。
2. 新建role, 修改绑定添加新role。角色聚合

以下示例修改role,  kubectl explain deploy属于apps/v1组的deploy资源 

```yaml 
kind: Role
apiVersion: rbac.authorization.k8s.io/v1 # 组/版本
metadata:
  namespace: default
  name: res-reader
rules:
- apiGroups: ["", "apps/v1"] # deployments属于apps/v1组   , 可能是较早的组extensions/v1beta1, 或者直接引用所有组["*"]
  resources: ["pods", "services", "deployments"] # 添加deployments 
  verbs: ["get", "list", "watch"] 
```

```bash
# 应用
apply
```

现在可以获取deploy, 但是不能删除pod

```bash
kubectl get deploy --kubeconfig=/tmp/ilinux.kubeconfig
# 成功

kubectl delete pod/pod1 --kubeconfig=/tmp/ilinux.kubeconfig
# 失败
```

修改role

```bash
kind: Role
apiVersion: rbac.authorization.k8s.io/v1 # 组/版本
metadata:
  namespace: default
  name: res-reader
rules:
- apiGroups: ["*"]
  resources: ["pods", "services", "deployments"] # 添加deployments 
  verbs: ["get", "list", "watch","delete"]
```

```bash
# 应用
apply


# 验证
kubectl delete pod/pod1 --kubeconfig=/tmp/ilinux.kubeconfig
#成功
```



### clusterrole + rolebinding

**clusterrole** 

内置集群角色，让组件获得权限

system:coredns

system:kube-dns

system:node 节点kubelet

system:kube-scheduler

system:volume-scheduler 

cluster-admin 只要绑定用户/组至这个角色就是集群管理员。+ 

> ```bash
> kubectl get clusterrolebinding/cluster-admin -o yaml
> subjects:
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group # 组
>   name: system:masters # openssl /O=system:masters
> 
> # 此命令查看O为管理员的用户
> [root@master kubernetes]# for i in `find . -name "*.crt"`; do  echo -n "$i          "; openssl x509 -noout -subject  -in $i ;done  | column -t | sort -k3
> ./pki/etcd/ca.crt                                                subject=  /CN=etcd-ca
> ./tmp/kubeadm-init-dryrun484176618/etcd/ca.crt                   subject=  /CN=etcd-ca
> ./pki/front-proxy-ca.crt                                         subject=  /CN=front-proxy-ca
> ./tmp/kubeadm-init-dryrun484176618/front-proxy-ca.crt            subject=  /CN=front-proxy-ca
> ./pki/front-proxy-client.crt                                     subject=  /CN=front-proxy-client
> ./tmp/kubeadm-init-dryrun484176618/front-proxy-client.crt        subject=  /CN=front-proxy-client
> ./pki/ilinux.crt                                                 subject=  /CN=ilinux/O=kubeusers
> ./pki/apiserver.crt                                              subject=  /CN=kube-apiserver
> ./tmp/kubeadm-init-dryrun484176618/apiserver.crt                 subject=  /CN=kube-apiserver
> ./pki/ca.crt                                                     subject=  /CN=kubernetes
> ./tmp/kubeadm-init-dryrun484176618/ca.crt                        subject=  /CN=kubernetes
> ./pki/etcd/peer.crt                                              subject=  /CN=master.magedu.com
> ./pki/etcd/server.crt                                            subject=  /CN=master.magedu.com
> ./tmp/kubeadm-init-dryrun484176618/etcd/peer.crt                 subject=  /CN=master.magedu.com
> ./tmp/kubeadm-init-dryrun484176618/etcd/server.crt               subject=  /CN=master.magedu.com
> ./pki/apiserver-etcd-client.crt                                  subject=  /O=system:masters/CN=kube-apiserver-etcd-client
> ./tmp/kubeadm-init-dryrun484176618/apiserver-etcd-client.crt     subject=  /O=system:masters/CN=kube-apiserver-etcd-client
> ./pki/apiserver-kubelet-client.crt                               subject=  /O=system:masters/CN=kube-apiserver-kubelet-client
> ./tmp/kubeadm-init-dryrun484176618/apiserver-kubelet-client.crt  subject=  /O=system:masters/CN=kube-apiserver-kubelet-client
> ./pki/etcd/healthcheck-client.crt                                subject=  /O=system:masters/CN=kube-etcd-healthcheck-client
> ./tmp/kubeadm-init-dryrun484176618/etcd/healthcheck-client.crt   subject=  /O=system:masters/CN=kube-etcd-healthcheck-client
> 
> ```

cluster-res-reader + rolebinding

| Default ClusterRole | Default ClusterRoleBinding | Description                                |
| ------------------- | -------------------------- | ------------------------------------------ |
| **cluster-admin**   | **system:masters** group   | clusterrolebinding/rolebinding。集群管理员 |
| **admin**           | None                       | rolebinding。名称空间管理员                |
| **edit**            | None                       | clusterrolebinding/rolebinding，可写       |
| **view**            | None                       | clusterrolebinding/rolebinding，只读       |

### Core component roles



内置**clusterrolebinding**

kubeadm:node-proxier  kube-proxy专用绑定

kubeadm:kubelet-bootstrap 构建集群使用的绑定

system:kube-controller-manager

system:scheduler

system:node

删除内建角色或绑定，重启apiserver自动生成



```bash
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1 
metadata:
  #clusterROle不属于名称空间
  name: cluster-res-reader
rules:
- apiGroups: ["*"]
  resources: ["pods", "services", "deployments"] # 添加deployments 
  verbs: ["get", "list", "watch","delete"]
```

ilinux对ingress-nginx名称空间不能操作, 因为role只对默认空间资源有效

```bash
#应用
apply
#获取 
kubectl get clustrrole  --kubeconfig=/tmp/ilinux.kubeconfig

#资源
kubectl get pods --kubeconfig=/tmp/ilinux.kubeconfig

#
```

绑定ilinux对ingress-nginx空间有权限，

## 集群角色绑定

不属于名称空间

```bash
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ilinux-res-reader #可以与RoleBinding同名，因为不同类型
  # 引用角色不使用名称空间
subjects:
- kind: User
  name: ilinux
  # 不需要写api群组
  # 不需要namespace
roleRef:
  kind: ClusterRole
  name: cluster-res-reader
  apiGroup: rbac.authorization.k8s.io
```

验证

```bash
kubectl get clusterrolebinding
# 不属于名称空间
```

 现在任何名称空间均可以获取资源



## 授权整个集群管理

- O=system:master
- cluster-admin + clusterrolebinding

```bash
kubectl create clusterrolebinding ilinux-cluster-admin --clusterrole=cluster-admin --user=ilinux
```

此时ilinux拥有所有权限

```bash
[root@master kubernetes]# kubectl get pod -n kube-system --kubeconfig=/tmp/ilinux.kubeconfig 
```

## 授权一个名称空间读

- view + rolebinding

```bash
[root@master kubernetes]# kubectl create rolebinding slc-system-reader --clusterrole=view --user=slc --namespace=kube-system
```

> 利用以上创建账户生成slc, 然后只读kube-system名称空间

根据view集群角色查看怎么定义全局读

```bash
[root@master ~]# kubectl get clusterrole view -o yaml

rules:
- apiGroups:
  - ""
  resources: # cm, endpoints, pvc, pvc/status, pod, rs, rs/scale, svc, sa, svc/status
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - persistentvolumeclaims/status
  - pods
  - replicationcontrollers
  - replicationcontrollers/scale
  - serviceaccounts
  - services
  - services/status
  verbs: # view动作
  - get
  - list
  - watch
- apiGroups:
  - "" # 核心组core
  resources: # binding, events, limitrages, ns/status, pod/log, pod/status, rc/status, resourcequotas, resourcequotas/status
  - bindings
  - events
  - limitranges
  - namespaces/status
  - pods/log
  - pods/status
  - replicationcontrollers/status
  - resourcequotas
  - resourcequotas/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources: # ns
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources: # daemonset, daemonset/status, deploy, deploy/status, rs, rs/scale, rs/status, ss, ss/scale, ss/status
  - controllerrevisions
  - daemonsets
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - replicasets
  - replicasets/scale
  - replicasets/status
  - statefulsets
  - statefulsets/scale
  - statefulsets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources: # hpa, hpa/status
  - horizontalpodautoscalers
  - horizontalpodautoscalers/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources: # cronjob, cronjob/status, jobs, jobs/status
  - cronjobs
  - cronjobs/status
  - jobs
  - jobs/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources: # daemonset, daemonset/status, deploy, deploy/status, ingress, ingress/status, networkpolicies, rs, rs/scale, rs/status, rc/scale
  - daemonsets 
  - daemonsets/status
  - deployments
  - deployments/scale
  - deployments/status
  - ingresses
  - ingresses/status
  - networkpolicies
  - replicasets
  - replicasets/scale
  - replicasets/status
  - replicationcontrollers/scale
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources: # pod disruption budgets
  - poddisruptionbudgets
  - poddisruptionbudgets/status
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources: # ingress, ingress/status, networkpolicies
  - ingresses
  - ingresses/status
  - networkpolicies
  verbs:
  - get
  - list
  - watch

```



# 二进制部署K8S集群

- 4套证书, 并且特定用户

  - etcd
  - kube-apiserver
  - kube-proxy
  - sa

  组件对应的用户必须一致

  https://kubernetes.io/docs/reference/access-authn-authz/rbac/#core-component-roles

  | Default ClusterRole                | Default ClusterRoleBinding              | Description                                                  |
  | ---------------------------------- | --------------------------------------- | ------------------------------------------------------------ |
  | **system:kube-scheduler**          | **system:kube-scheduler** user          | 内建绑定 system:kube-scheduler用户至system:kube-scheduler角色。创建证书的CN=system:kube-scheduler，如果不是，CN必须绑定至system:kube-scheduler |
  | **system:volume-scheduler**        | **system:kube-scheduler** user          | scheduler也要绑定这个角色至用户                              |
  | **system:kube-controller-manager** | **system:kube-controller-manager** user | controller-manager                                           |
  | **system:node**                    | None                                    | 仅需要--authorization-mode=Node,RBAC                         |
  | **system:node-proxier**            | **system:kube-proxy** user              | kube-proxy的证书CN=system:kube-proxy，如果不是，CN必须绑定至system:node-proxier |

  ## 其他组件角色

  | Default ClusterRole                      | Default ClusterRoleBinding                                   | Description                                                  |
  | ---------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | **system:auth-delegator**                | None                                                         | 允许委托的身份验证和授权检查。 附加API服务器通常将其用于统一身份验证和授权。 |
  | **system:heapster**                      | None                                                         | Role for the [Heapster](https://github.com/kubernetes/heapster) component (deprecated). |
  | **system:kube-aggregator**               | None                                                         | Role for the [kube-aggregator](https://github.com/kubernetes/kube-aggregator) component. |
  | **system:kube-dns**                      | **kube-dns** service account in the **kube-system** namespace | Role for the [kube-dns](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) component. |
  | **system:kubelet-api-admin**             | None                                                         | Allows full access to the kubelet API.                       |
  | **system:node-bootstrapper**             | None                                                         | Allows access to the resources required to perform [kubelet TLS bootstrapping](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/). 基于引导令牌拉集群，必须创建用户绑定至这个角色 |
  | **system:node-problem-detector**         | None                                                         | Role for the [node-problem-detector](https://github.com/kubernetes/node-problem-detector) component. |
  | **system:persistent-volume-provisioner** | None                                                         | Allows access to the resources required by most [dynamic volume provisioners](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#provisioner). |
  | **system:monitoring**                    | **system:monitoring** group                                  | 允许对控制平面监视端点，即[kube-apiserver](https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver）活动和就绪端点（`/ healthz`，`/ livez`，`/ readyz`），各个健康检查端点（`/ healthz / *`，`/ livez / *`，`/ readyz / *`）和`/ metrics）。 请注意，各个运行状况检查端点和度量标准端点可能会公开敏感信息。 |



# dashboard

k8s著名的addon, k8s dashboard(面板)，即k8s的web ui, 图形界面查看pod, pod controller，dashboard是k8s的管理接口，就得精心组织k8s的权限。

多用户，不同的账号的用户登陆，有不同的权限。dashboard不进行认证，仅仅将认证请求代理至kuberentes.

- 全部k8s用户，权限即k8s的权限。
- 以pod运行于k8s之上，pod访问api server, 得使用serviceaccount, 意味着账号不能是user/group常规账号。
- 访问web ui, 流量来自集群的外，得使用service nodePort或ingress开放，http或https
  - https: 
    - ingress https(secret tls)
    - service/ingress -> dashboard https(secret, pod使用证书generic)



github -> kuberentes dashboard https://github.com/kubernetes/dashboard



部署方式

```diff
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.1.0/aio/deploy/recommended.yaml


# yaml
+#挂载certs目录，使用kubernetes-dashboard-certs的secret来完成https通信，所以这个secrets生成generic的证书要互联网ca信任的证书
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.1.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            # Uncomment the following line to manually specify Kubernetes API server Host
            # If not specified, Dashboard will attempt to auto discover the API server and connect
            # to it. Uncomment only if the default does not work.
            # - --apiserver-host=http://my-address:port
          volumeMounts:
+            - name: kubernetes-dashboard-certs
+              mountPath: /certs
              # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
+        - name: kubernetes-dashboard-certs
          secret:
+            secretName: kubernetes-dashboard-certs
apiVersion: v1
+kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard 
+  name: kubernetes-dashboard-certs # kubernetes-dashboard名称空间中有这个 Opaque类型(generic)证书, 让k8s ca签署的或公共信任的证书。用于tls通信的证书
  namespace: kubernetes-dashboard
type: Opaque


+# kubernetes-dashboard名称空间中的secrets, cm, service获取get,权限
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
+  namespace: kubernetes-dashboard
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
+    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
+    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
+    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
+    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
+    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
+    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
+    verbs: ["get"]

+#集群中的pods, nodes资源查看权限
---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
+    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]
    
    
---
+# kubernetes-dashboard名称空间的sa绑定至kubernetes-dashboard名称空间的role。绑定放在kubernetes-dashboard名称空间
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
+  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
+    name: kubernetes-dashboard
+    namespace: kubernetes-dashboard

---
+# kubernetes-dashboard名称空间的sa绑定至kubernetes-dashboard集群角色
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
+  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
+    name: kubernetes-dashboard
+    namespace: kubernetes-dashboard



---
+#service ip:443 ->  k8s-app: kubernetes-dashboard标签选择pod的8443
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
```

tls通信的证书生成

```bash
openssl genrsa -out dashboard.key 2048

openssl req -new -key dashboard.key -out dashboard.csr -subj "/CN=ui.ilinux.io/O=ilinux"
```

> CN证书检验

```bash
cd /etc/kubernetes/pki/
openssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 3650
```

> 应该正常使用互联网的CA来签署，这个仅使用dashboardtls通信，
>
> service4层 -> dashboard ssl

创建generic

```bash
kubectl create secret generic kubernetes-dashboard-certs -n kubernetes-dashboard --from-file=dashboard.crt --from-file=dashboard.key

# 验证
kubectl get secret -n kubernetes-dashboard

# yaml
kubectl get secret -n kubernetes-dashboard -o yaml
```

> 键必须为dashboard.crt,dashboard.key, 如果文件名不是这个键就不一样，需要手工指定键名。

## dashboard https

4层 -> dashboard svc -> dashboard(https)

```bash
# dashboard svc
[root@k8s-master1 ~]# kubectl get svc -A | grep dashboard
kube-system   dashboard-metrics-scraper                             ClusterIP   10.79.238.51    <none>        8000/TCP                                         138d
kube-system   kubernetes-dashboard                                  NodePort    10.79.126.26    <none>        443:30002/TCP                                    138d
```

获取dashboard svc地址

```bash
kubernetes-dashboard.kube-system.svc.weizhixiu.local.
```

在nginx pod中测试

```bash
[root@k8s-master1 ~]# kubectl exec -it -n weizhixiu     weizhixiu-nginx-ingress-nginx-deployment-7c666c4688-xgspk -- bash
[root@weizhixiu-nginx-ingress-nginx-deployment-7c666c4688-xgspk /]# curl -k   https://kubernetes-dashboard.kube-system.svc.weizhixiu.local    
```

阿里云解析域名

![image-20210201104207478](http://myapp.img.mykernel.cn/image-20210201104207478.png)

```bash
# 测试解析
[root@wzx ~]# ping dashboard.youninyouqu.com
PING dashboard.youninyouqu.com (121.40.52.138) 56(84) bytes of data.
```



阿里云生成证书

![image-20210201104436096](http://myapp.img.mykernel.cn/image-20210201104436096.png)

![image-20210201104619517](http://myapp.img.mykernel.cn/image-20210201104619517.png)

![image-20210201104706409](http://myapp.img.mykernel.cn/image-20210201104706409.png)

## 生成证书

```bash
[root@k8s-master1 ssl]# unzip 5143627_dashboard.youninyouqu.com_other.zip 
Archive:  5143627_dashboard.youninyouqu.com_other.zip
Aliyun Certificate Download
  inflating: 5143627_dashboard.youninyouqu.com.pem  
  inflating: 5143627_dashboard.youninyouqu.com.key  

# 获取证书
# kubectl get deploy -n kube-system kubernetes-dashboard -o yaml
        volumeMounts:
        - mountPath: /certs
          name: kubernetes-dashboard-certs

	volumes:
      - name: kubernetes-dashboard-certs
        secret:
          defaultMode: 420
          secretName: kubernetes-dashboard-certs

# 生成证书 kube-system.kubernetes-dashboard-certs
[root@k8s-master1 ssl]# kubectl delete secret   kubernetes-dashboard-certs -n kube-system
secret "kubernetes-dashboard-certs" deleted
[root@k8s-master1 ssl]# kubectl create secret generic kubernetes-dashboard-certs -n kube-system --from-file=dashboard.key=5143627_dashboard.youninyouqu.com.key --from-file=dashboard.crt=5143627_dashboard.youninyouqu.com.pem 
secret/kubernetes-dashboard-certs created
[root@k8s-master1 ssl]# 
[root@k8s-master1 ssl]# kubectl get secret   kubernetes-dashboard-certs -n kube-system -o yaml
apiVersion: v1
data:
  dashboard.crt:  # key没有问题
  dashboard.key: 
kind: Secret
metadata:
  creationTimestamp: "2021-02-01T02:53:41Z"
  name: kubernetes-dashboard-certs
  namespace: kube-system
  resourceVersion: "33557901"
  selfLink: /api/v1/namespaces/kube-system/secrets/kubernetes-dashboard-certs
  uid: 8fbc815a-2369-470c-9c6a-0d90c6a43c77
type: Opaque
```

虽然pod可以实时反应挂载的secret, 但是进程没有重读配置

```bash
# 重读配置
[root@k8s-master1 ingress-nginx]# kubectl delete pods -n kube-system   kubernetes-dashboard-5795555b68-rcggq
```

浏览器访问: 

https://dashboard.youinyouqu.com:30002

失败

```bash
[root@k8s-master1 ~]# kubectl edit deploy -n kube-system kubernetes-dashboard

    spec:
      containers:
      - args:
        - --auto-generate-certificates
        - --namespace=kube-system
        - --tls-cert-file=tls.crt # 只能相对路径，dashboard自动加载/certs/tls.crt
        - --tls-key-file=tls.key
        volumeMounts:
        - mountPath: /certs
          name: kubernetes-dashboard-certs
        - mountPath: /tmp
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          defaultMode: 420
          items:
          - key: dashboard.key # 挂载的key
            path: tls.key      # 文件名
          - key: dashboard.crt
            path: tls.crt
          secretName: kubernetes-dashboard-certs
      - emptyDir: {}
        name: tmp-volume
[root@k8s-master1 ~]# kubectl delete pods -n kube-system   kubernetes-dashboard-5bd9b4bb6c-qs6gn

```

浏览器访问https://dashboard.youninyouqu.com:30002/#/login

![image-20210201113841878](http://myapp.img.mykernel.cn/image-20210201113841878.png)

## clusterrolebinding + clusterrole(cluster-admin) + sa用户

授权操作k8s的是 dashboard pod中的进程有权限，所以授权给serviceaccount用户



创建sa用户

```bash
[root@k8s-master1 ~]# kubectl create serviceaccount ui-admin -n kube-system # 管理员放的位置
serviceaccount/ui-admin created

# 获取用户的token
[root@k8s-master1 ~]# kubectl get -n kube-system sa ui-admin -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: "2021-02-01T03:42:07Z"
  name: ui-admin
  namespace: kube-system
  resourceVersion: "33567453"
  selfLink: /api/v1/namespaces/kube-system/serviceaccounts/ui-admin
  uid: 69f9ced9-c4a5-4f78-9d0b-7d121780c34e
secrets:
- name: ui-admin-token-l9rsd # secret

# token
[root@k8s-master1 ~]# kubectl get secret  ui-admin-token-l9rsd -n kube-system -o yaml
apiVersion: v1
data:
  ca.crt: 
  namespace: a3ViZS1zeXN0ZW0=
  token: 

# base64解码或describe..
echo '' | base64 -d

```

> kube-system名称空间，只是账号位置，并非是账号的权限应用于哪个名称空间

管理kube-system名称空间

用户 + rolebinding(kubesystem) + cluster-admin

```bash
[root@k8s-master1 ~]# kubectl create -n kube-system rolebinding ui-admin-rb --clusterrole=admin --serviceaccount=kube-system:ui-admin
rolebinding.rbac.authorization.k8s.io/ui-admin-rb created

```

管理testing名称空间

```bash
kubectl create -n default rolebinding ui-admin-rb --clusterrole=admin --serviceaccount=kube-system:ui-admin
```

> sa的位置和管理哪个名称空间无关，rolebinding的位置才是

### 查看名称空间

可以管理default名称空间，但是看不到其他名称空间，授权查看名称空间

```bash
kubectl create clusterrole managerns --verb=get,list,watch --resource=namespaces
kubectl create clusterrolebinding ui-admin-managerns --clusterrole=managerns --serviceaccount=kube-system:ui-admin
```

清理绑定

```bash
# 查看default名称空间
kubectl delete -n default rolebinding ui-admin-rb
# 查看kube-system名称空间
kubectl delete -n kube-system rolebinding ui-admin-rb
# 查看ns
kubectl delete clusterrolebinding ui-admin-managerns
```



## 授权ui-admin有集群管理员权限

```bash
kubectl create clusterrolebinding ui-admin-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:ui-admin
```

清理集群管理权限

```bash
kubectl delete clusterrolebinding ui-admin-cluster-admin 
```



# 基于service的token生成kubeconfig文件

避免每次使用token登陆

```bash
 kubectl describe secret ui-admin -n kube-system
token:     xxxx
```

## kubeconfig使用token生成

生成文件，设定集群

```bash
kubectl config  set-cluster  ui-admin-config --server="https://172.16.0.222:6443" --certificate-authority=/etc/kubernetes/ssl/ca.pem  --embed-certs=true  --kubeconfig=ui-admin-config
```

设定访问集群的用户，基于token生成，**不需要用户名**

```bash
kubectl config set-credentials ui-admin-user --token= --kubeconfig=ui-admin-config
```

> token即为上面`kubectl describe secret ui-admin -n kube-system`的token值。base64解密结果

设定上下文

```bash
kubectl config set-context ui-admin-user@ui-admin-config --cluster=ui-admin-config --user=ui-admin-user  --kubeconfig=ui-admin-config

kubectl config use-context ui-admin-user@ui-admin-config --kubeconfig=ui-admin-config
```



## 测试正常

```bash
[root@k8s-master1 ~]# kubectl get pod --kubeconfig=ui-admin-config
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-75d6dbfb47-wfq5n   1/1     Running   0          83d
nginx-deployment-5d66cc795f-d6bqx         1/1     Running   0          83d
test-6778c74665-q2qp4                     1/1     Running   0          83d
```

